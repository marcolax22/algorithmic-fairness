{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract probabilities and labels for male and female applicants\n",
    "# male_probs = y_probs[X_test['gender'] == 1]\n",
    "# female_probs = y_probs[X_test['gender'] == 0]\n",
    "\n",
    "# # True labels for each gender group\n",
    "# male_labels = y_test[X_test['gender'] == 1]\n",
    "# female_labels = y_test[X_test['gender'] == 0]\n",
    "\n",
    "# # Compute ROC curves for both groups\n",
    "# fpr_male, tpr_male, _ = roc_curve(male_labels, male_probs)\n",
    "# fpr_female, tpr_female, _ = roc_curve(female_labels, female_probs)\n",
    "\n",
    "# # Compute AUC for both groups\n",
    "# auc_male = auc(fpr_male, tpr_male)\n",
    "# auc_female = auc(fpr_female, tpr_female)\n",
    "\n",
    "# # Plot ROC curves for both groups\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr_male, tpr_male, color='blue', label=f'Male (AUC = {auc_male:.2f})')\n",
    "# plt.plot(fpr_female, tpr_female, color='red', label=f'Female (AUC = {auc_female:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curves for Male and Female')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# import packages\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# import scripts\n",
    "from data_processing import hiring_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to X and y\n",
    "X = hiring_data.drop(columns=['decision', 'Id', 'company', 'ind-exact_study'])\n",
    "y = hiring_data['decision']\n",
    "\n",
    "# test train set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove gender from training features\n",
    "X_train_no_gender = X_train.drop(columns=['gender'])\n",
    "X_test_no_gender = X_test.drop(columns=['gender'])\n",
    "\n",
    "# Train the logistic regression model WITHOUT gender\n",
    "model = LogisticRegression(max_iter=4000)\n",
    "model_standard = model.fit(X_train_no_gender, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities for the positive class (decision = 1)\n",
    "y_probs = model_standard.predict_proba(X_test_no_gender)[:, 1]\n",
    "y_pred = model_standard.predict(X_test_no_gender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selection rate by gender:\n",
      "gender\n",
      "0    0.114613\n",
      "1    0.230068\n",
      "2    0.416667\n",
      "Name: prediction, dtype: float64\n",
      "\n",
      "Count of selections by gender:\n",
      "gender  prediction\n",
      "0       0             309\n",
      "        1              40\n",
      "1       0             338\n",
      "        1             101\n",
      "2       0               7\n",
      "        1               5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confusion Matrix:\n",
      "[[505  62]\n",
      " [149  84]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.89      0.83       567\n",
      "           1       0.58      0.36      0.44       233\n",
      "\n",
      "    accuracy                           0.74       800\n",
      "   macro avg       0.67      0.63      0.64       800\n",
      "weighted avg       0.71      0.74      0.72       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with predictions and actual labels\n",
    "results = pd.DataFrame({'gender': X_test['gender'], 'prediction': y_pred, 'actual': y_test})\n",
    "\n",
    "# Count selection rates by gender\n",
    "selection_rate_by_gender = results.groupby('gender')['prediction'].mean()\n",
    "print(\"\\nSelection rate by gender:\")\n",
    "print(selection_rate_by_gender)\n",
    "\n",
    "# Display counts of predictions per gender\n",
    "print(\"\\nCount of selections by gender:\")\n",
    "print(results.groupby('gender')['prediction'].value_counts())\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Scores (F1, accuracy, precision, recall)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most important features:\n",
      "                 Feature  Coefficient\n",
      "8          ind-languages     1.210821\n",
      "9             ind-degree     0.757908\n",
      "6  ind-international_exp     0.446773\n",
      "5    ind-programming_exp    -0.332501\n",
      "7    ind-entrepeneur_exp    -0.268937\n",
      "4         ind-debateclub    -0.157348\n",
      "3   ind-university_grade     0.075180\n",
      "2                  sport    -0.040483\n",
      "1            nationality    -0.034473\n",
      "0                    age     0.010572\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get feature importance (absolute values of coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_no_gender.columns,\n",
    "    'Coefficient': model_standard.coef_[0]\n",
    "})\n",
    "\n",
    "# Sort by absolute coefficient value (importance)\n",
    "feature_importance['Abs_Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
    "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nMost important features:\")\n",
    "print(feature_importance[['Feature', 'Coefficient']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
